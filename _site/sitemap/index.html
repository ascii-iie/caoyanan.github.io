

<!doctype html>
<html lang="zh" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Sitemap - Shi Wang</title>







<meta property="og:locale" content="zh-CN">
<meta property="og:site_name" content="Shi Wang">
<meta property="og:title" content="Sitemap">


  <link rel="canonical" href="http://127.0.0.1:4000/sitemap/">
  <meta property="og:url" content="http://127.0.0.1:4000/sitemap/">


















  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Organization",
      "name" : "Shi Wang",
      "url" : "http://127.0.0.1:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://127.0.0.1:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Shi Wang Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://127.0.0.1:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="http://127.0.0.1:4000/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="http://127.0.0.1:4000/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="http://127.0.0.1:4000/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="http://127.0.0.1:4000/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="http://127.0.0.1:4000/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="http://127.0.0.1:4000/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="http://127.0.0.1:4000/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="http://127.0.0.1:4000/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="http://127.0.0.1:4000/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="http://127.0.0.1:4000/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="http://127.0.0.1:4000/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="http://127.0.0.1:4000/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="http://127.0.0.1:4000/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="http://127.0.0.1:4000/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="http://127.0.0.1:4000/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://127.0.0.1:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://127.0.0.1:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://127.0.0.1:4000/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://127.0.0.1:4000/">Shi Wang</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://127.0.0.1:4000/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://127.0.0.1:4000/talks/">Talks</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://127.0.0.1:4000/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://127.0.0.1:4000/portfolio/">Fundings</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://127.0.0.1:4000/year-archive/">Blog Posts</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://127.0.0.1:4000/images/profile.png" class="author__avatar" alt="Shi Wang at <a href='http://www.ict.ac.cn'>ICT/CAS</a>">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Shi Wang at <a href='http://www.ict.ac.cn'>ICT/CAS</a></h3>
    <p class="author__bio">KG / NLP / NSDP</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Beijing, China</li>
      
      
      
      
        <li><a href="mailto:wangshi@ict.ac.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/ICTKC"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=IAnjAfcAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <div class="archive">
    
      <h1 class="page__title">Sitemap</h1>
    
    
<p>A list of all the posts and pages found on the site. For you robots out there is an <a href="http://127.0.0.1:4000/sitemap.xml">XML version</a> available for digesting as well.</p>

<h2>Pages</h2>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/404.html" rel="permalink">Page Not Found</a>
      
    </h2>
    
    

        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Page not found. Your pixels are in another canvas.</p>
</p>
    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/" rel="permalink">Hi, this is Shi Wang ~</a>
      
    </h2>
    
    

        

    
    <p class="archive__item-excerpt" itemprop="description"><p>About me</p>
</p>
    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/archive-layout-with-content/" rel="permalink">Archive Layout with Content</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/categories/" rel="permalink">Posts by Category</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/collection-archive/" rel="permalink">Posts by Collection</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/cv/" rel="permalink">CV</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/assets/css/main.css" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/markdown/" rel="permalink">Markdown</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/non-menu-page/" rel="permalink">Page not in menu</a>
      
    </h2>
    
    

        

    
    <p class="archive__item-excerpt" itemprop="description"><p>This is a page not in th emain menu</p>
</p>
    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/page-archive/" rel="permalink">Page Archive</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/portfolio/" rel="permalink">Portfolio</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publications/" rel="permalink">Publications</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/sitemap/" rel="permalink">Sitemap</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/tags/" rel="permalink">Posts by Tags</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/talkmap.html" rel="permalink">Talk map</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/talks/" rel="permalink">Talks and presentations</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/teaching/" rel="permalink">Teaching</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/terms/" rel="permalink">Terms and Privacy Policy</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/year-archive/" rel="permalink">Blog posts</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/assets/css/style.css" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/about/" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/about.html" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/resume" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/md/" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/markdown.html" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/nmp/" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/nmp.html" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/wordpress/blog-posts/" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/redirects.json" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/markdown_generator/" rel="permalink">Jupyter notebook markdown generator</a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/feed.xml" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/sitemap.xml" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/robots.txt" rel="permalink"></a>
      
    </h2>
    
    

        

    
    
    

  </article>
</div>

<h2>Posts</h2>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/posts/2021/08/blog-post-4/" rel="permalink">happy new year 2022!
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2021-01-01T00:00:00-08:00">January 01, 2021</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>happy new year 2022!</p>
</p>
    
    
    

  </article>
</div>

<h2>portfolio</h2>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/portfolio/portfolio-1/" rel="permalink">Fundings
</a>
      
    </h2>
    
    

        

    
    <p class="archive__item-excerpt" itemprop="description"><p><img src="/images/1.png" /></p>
</p>
    
    
    

  </article>
</div>

<h2>publications</h2>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/more" rel="permalink">more…
</a>
      
    </h2>
    
    

        
          <p>Published in <i>26/04</i>, 1981 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    
      <p>Recommended citation: ^_^ </p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/EMNLP-18-Sun" rel="permalink">Answer-focused and Position-aware Neural Question Generation
</a>
      
    </h2>
    
    

        
          <p>Published in <i>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP-18)</i>, 2018 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>In this paper, we focus on the problem of question generation (QG). Recent neural network-based approaches employ the sequence-to-sequence model which takes an answer and its context as input and generates a relevant question as output. However, we observe two major issues with these approaches: (1) The generated interrogative words (or question words) do not match the answer type. (2) The model copies the context words that are far from and irrelevant to the answer, instead of the words that are close and relevant to the answer. To address these two issues, we propose an answer-focused and position-aware neural question generation model. (1) By answer-focused, we mean that we explicitly model question word generation by incorporating the answer embedding, which can help generate an interrogative word matching the answer type. (2) By position-aware, we mean that we model the relative distance between the context words and the answer. Hence the model can be aware of the position of the context words when copying them to generate a question. We conduct extensive experiments to examine the effectiveness of our model. The experimental results show that our model significantly improves the baseline and outperforms the state-of-the-art system.</p>
</p>
    
    
    
      <p>Recommended citation: Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, Shi Wang: Answer-focused and Position-aware Neural Question Generation. EMNLP 2018: 3930-3939 </p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/WWW-20-FangZheng" rel="permalink">High Quality Candidate Generation and Sequential Graph Attention Network for Entity Linking
</a>
      
    </h2>
    
    

        
          <p>Published in <i>Proceedings of The Web Conference 2020 (WWW-20)</i>, 2020 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Entity Linking (EL) is a task for mapping mentions in text to corresponding entities in knowledge base (KB). This task usually includes candidate generation (CG) and entity disambiguation (ED) stages. Recent EL systems based on neural network models have achieved good performance, but they still face two challenges: (i) Previous studies evaluate their models without considering the differences between candidate entities. In fact, the quality (gold recall in particular) of candidate sets has an effect on the EL results. So, how to promote the quality of candidates needs more attention. (ii) In order to utilize the topical coherence among the referred entities, many graph and sequence models are proposed for collective ED. However, graph-based models treat all candidate entities equally which may introduce much noise information. On the contrary, sequence models can only observe previous referred entities, ignoring the relevance between the current mention and its subsequent entities. To address the first problem, we propose a multi-strategy based CG method to generate high recall candidate sets. For the second problem, we design a Sequential Graph Attention Network (SeqGAT) which combines the advantages of graph and sequence methods. In our model, mentions are dealt with in a sequence manner. Given the current mention, SeqGAT dynamically encodes both its previous referred entities and subsequent ones, and assign different importance to these entities. In this way, it not only makes full use of the topical consistency, but also reduce noise interference. We conduct experiments on different types of datasets and compare our method with previous EL system on the open evaluation platform. The comparison results show that our model achieves significant improvements over the state-of-the-art methods.</p>
</p>
    
    
    
      <p>Recommended citation: Zheng Fang, Yanan Cao, Ren Li, Zhenyu Zhang, Yanbing Liu, Shi Wang: High Quality Candidate Generation and Sequential Graph Attention Network for Entity Linking. WWW 2020: 640-650  </p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/PAKDD-20-TangHengzhu" rel="permalink">HIN: Hierarchical Inference Network for Document-Level Relation Extraction (BEST PAPER AWARD)
</a>
      
    </h2>
    
    

        
          <p>Published in <i>Proceedings of the 2020 Conference on Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD-20)</i>, 2020 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Document-level RE requires reading, inferring and aggregating over multiple sentences. From our point of view, it is necessary for document-level RE to take advantage of multi-granularity inference information: entity level, sentence level and document level. Thus, how to obtain and aggregate the inference information with different granularity is challenging for document-level RE, which has not been considered by previous work. In this paper, we propose a Hierarchical Inference Network (HIN) to make full use of the abundant information from entity level, sentence level and document level. Translation constraint and bilinear transformation are applied to target entity pair in multiple subspaces to get entity-level inference information. Next, we model the inference between entity-level information and sentence representation to achieve sentence-level inference information. Finally, a hierarchical aggregation approach is adopted to obtain the document-level inference information. In this way, our model can effectively aggregate inference information from these three different granularities. Experimental results show that our method achieves state-of-the-art performance on the large-scale DocRED dataset. We also demonstrate that using BERT representations can further substantially boost the performance.</p>
</p>
    
    
    
      <p>Recommended citation: Hengzhu Tang, Yanan Cao, Zhenyu Zhang, Jiangxia Cao, Fang Fang, Shi Wang, Pengfei Yin: HIN: Hierarchical Inference Network for Document-Level Relation Extraction. PAKDD (1) 2020: 197-209  </p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/EMNLP-20-Jia" rel="permalink">Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network
</a>
      
    </h2>
    
    

        
          <p>Published in <i>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP-20)</i>, 2020 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences. Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing. Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.</p>
</p>
    
    
    
      <p>Recommended citation: Ruipeng Jia, Yanan Cao, Hengzhu Tang, Fang Fang, Cong Cao, Shi Wang:Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network. EMNLP (1) 2020: 3622-3631  </p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/AAAI-21-Jia" rel="permalink">Flexible Non-Autoregressive Extractive Summarization with Threshold: How to Extract a Non-Fixed Number of Summary Sentences
</a>
      
    </h2>
    
    

        
          <p>Published in <i>Proceedings of The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)</i>, 2021 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Sentence-level extractive summarization is a fundamental yet challenging task, and recent powerful approaches prefer to pick sentences sorted by the predicted probabilities until the length limit is reached, a.k.a. Top-K Strategy. This length limit is fixed based on the validation set, resulting in the lack of flexibility. In this work, we propose a more flexible and accurate non-autoregressive method for single document extractive summarization, extracting a non-fixed number of summary sentences without the sorting step. We call our approach ThresSum as it picks sentences simultaneously and individually from the source document when the predicted probabilities exceed a threshold. During training, the model enhances sentence representation through iterative refinement and the intermediate latent variables receive some weak supervision with soft labels, which are generated progressively by adjusting the temperature with a knowledge distillation algorithm. Specifically, the temperature is initialized with high value and drops along with the iteration until a temperature of 1. Experimental results on CNN/DM and NYT datasets have demonstrated the effectiveness of ThresSum, which significantly outperforms BERTSUMEXT with a substantial improvement of 0.74 ROUGE-1 score on CNN/DM. Our source code will be available on Github.</p>
</p>
    
    
    
      <p>Recommended citation: Ruipeng Jia, Yanan Cao, Haichao Shi, Fang Fang, Pengfei Yin, Shi Wang: Flexible Non-Autoregressive Extractive Summarization with Threshold: How to Extract a Non-Fixed Number of Summary Sentences. AAAI 2021: 13134-13142 </p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/IJCNN-21-LiuYu" rel="permalink">Knowledge Enhanced Sequential Entity Linking
</a>
      
    </h2>
    
    

        
          <p>Published in <i>2021 International Joint Conference on Neural Networks (IJCNN-21)</i>, 2021 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Entity Linking (EL) is the task of mapping mentions in texts to the corresponding entities in knowledge bases. Existing studies mostly focus on joint disambiguation based on the topical coherence, including graph and sequence models. Sequence models alleviate the complexity caused by graph models, but exist the error propagation that incorrectly disambiguated entities are likely to induce further errors when predicting future mentions. Moreover, it is a huge expense to construct the relationship be- tween entities to explore structured knowledge. To address these problems, we propose a novel method, Knowledge Enhanced Se- quential Entity Linking (KESEL), which converts global EL into a sequence decision problem and applies a pre-trained language model to better fuse entity knowledge. Specifically, we firstly uti- lize multiple features to learn local contextual representations of mentions and candidates respectively. Next, a sequential ERNIE model is introduced to generate knowledgeable representations by dynamically integrating the knowledge of previously referred entities into subsequent mentions disambiguation. Finally, by concatenating the above learned contextual and knowledgeable representations, we make full use of multi-semantic information to improve the performance of EL. Extensive experiments show that our method can achieve competitive or state-of-the-art results.</p>
</p>
    
    
    
      <p>Recommended citation: Yu Liu, Shi Wang, Kangli Zi, Jicun Li, Cungen Cao: Knowledge Enhanced Sequential Entity Linking. IJCNN 2021: 1-8 </p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/ACL-21-Jia" rel="permalink">Deep Differential Amplifier for Extractive Summarization
</a>
      
    </h2>
    
    

        
          <p>Published in <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (ACL-21)</i>, 2021 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>For sentence-level extractive summarization, there is a disproportionate ratio of selected and unselected sentences, leading to flatting the summary features when maximizing the accuracy. In this paper, we conceptualize the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework. Specifically, we first calculate and amplify the semantic difference between each sentence and all other sentences, and then apply the residual unit as the second item of the differential amplifier to deepen the architecture. Finally, to compensate for the imbalance, the corresponding objective loss of minority class is boosted by a weighted cross-entropy. In contrast to previous approaches, this model pays more attention to the pivotal information of one sentence, instead of all the informative context modeling by recurrent or Transformer architecture. We demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods. Our source code will be available on Github.</p>
</p>
    
    
    
      <p>Recommended citation: Ruipeng Jia, Yanan Cao, Fang Fang, Yuchen Zhou, Zheng Fang, Yanbing Liu, Shi Wang: Deep Differential Amplifier for Extractive Summarization. ACL/IJCNLP (1) 2021: 366-376 </p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/EMNLP-21-Sun" rel="permalink">Enhancing Document Ranking with Task-adaptive Training and Segmented Token Recovery Mechanism
</a>
      
    </h2>
    
    

        
          <p>Published in <i>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP-21)</i>, 2021 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>In this paper, we propose a new ranking model DR-BERT, which improves the Document Retrieval (DR) task by a task-adaptive training process and a Segmented Token Recovery Mechanism (STRM). In the task-adaptive training, we first pre-train DR-BERT to be domain-adaptive and then make the two-phase fine-tuning. In the first-phase fine-tuning, the model learns query-document matching patterns regarding different query types in a pointwise way. Next, in the second-phase fine-tuning, the model learns document-level ranking features and ranks documents with regard to a given query in a listwise manner. Such pointwise plus listwise fine-tuning enables the model to minimize errors in the document ranking by incorporating ranking-specific supervisions. Meanwhile, the model derived from pointwise fine-tuning is also used to reduce noise in the training data of the listwise fine-tuning. On the other hand, we present STRM which can compute OOV word representation and contextualization more precisely in BERT-based models. As an effective strategy in DR-BERT, STRM improves the matching perfromance of OOV words between a query and a document. Notably, our DR-BERT model keeps in the top three on the MS MARCO leaderboard since May 20, 2020.</p>
</p>
    
    
    
      <p>Recommended citation: Xingwu Sun, Yanling Cui, Hongyin Tang, Fuzheng Zhang, Beihong Jin, Shi Wang: Enhancing Document Ranking with Task-adaptive Training and Segmented Token Recovery Mechanism. EMNLP (1) 2021: 3570-3579 </p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/publication/EMNLP-21-zi" rel="permalink">SOM-NCSCM : An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map
</a>
      
    </h2>
    
    

        
          <p>Published in <i>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP-21)</i>, 2021 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Sentence Compression (SC), which aims to shorten sentences while retaining important words that express the essential meanings, has been studied for many years in many languages, especially in English. However, improvements on Chinese SC task are still quite few due to several difficulties: scarce of parallel corpora, different segmentation granularity of Chinese sentences, and imperfect performance of syntactic analyses. Furthermore, entire neural Chinese SC models have been under-investigated so far. In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answering system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. Experimental results show that our SOM-NCSCM can significantly benefit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on Chinese SC task.</p>
</p>
    
    
    
      <p>Recommended citation: Kangli Zi, Shi Wang, Yu Liu, Jicun Li, Yanan Cao, Cungen Cao: SOM-NCSCM : An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map. EMNLP (1) 2021: 403-415 </p>
    

  </article>
</div>

<h2>talks</h2>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/talks/2019-06-21-qa" rel="permalink">Question Answering and Some Deep Learning Methods Involved
</a>
      
    </h2>
    
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2019-06-21T00:00:00-07:00">June 21, 2019</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>An overview of Question Answering and Some Deep Learning Methods Involved</p>

</p>
    
    
    

  </article>
</div>

<h2>teaching</h2>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://127.0.0.1:4000/teaching/2019-summer-teaching-1" rel="permalink">Question Answering and Some Deep Learning Methods Involved
</a>
      
    </h2>
    
    

        
          <p> Workshop, <i>University 1, Department</i>, 2019 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>An overview of Question Answering and Some Deep Learning Methods Involved</p>

</p>
    
    
    

  </article>
</div>


  </div>
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
    
    
    
      <li><a href="http://github.com/ICTKC"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://127.0.0.1:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Shi Wang. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="http://127.0.0.1:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

